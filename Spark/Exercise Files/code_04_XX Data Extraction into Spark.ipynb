{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18a4cfe3-b04e-46c3-86d3-db9f28cd44ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/06 19:00:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark is ready. Version: 4.0.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/Library/Java/JavaVirtualMachines/openjdk-17.jdk/Contents/Home\"\n",
    "\n",
    "# Import findspark to help Jupyter locate your Spark installation\n",
    "import findspark\n",
    "\n",
    "# Initialize the findspark library — sets up environment so Spark works in notebooks\n",
    "findspark.init()\n",
    "\n",
    "# Import SparkSession, the main entry point to Spark functionality\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder # Start Spark session builder\n",
    "            .appName(\"SparkReadJob\") # Set application name\n",
    "            .config(\"spark.sql.shuffle.partitions\", 2) # Set number of shuffle partitions (e.g. for groupBy, joins)\n",
    "            .config(\"spark.default.parallelism\", 2) # Set default parallelism\n",
    "            .config(\"spark.sql.warehouse.dir\", \"spark-warehouse\") # Set warehouse directory\n",
    "            .config(\"spark.driver.extraJavaOptions\", \"--add-opens java.base/javax.security.auth=ALL-UNNAMED\")\n",
    "            .config(\"spark.executor.extraJavaOptions\", \"--add-opens java.base/javax.security.auth=ALL-UNNAMED\")\n",
    "            .enableHiveSupport() # Enable Hive support for Spark SQL\n",
    "            .master(\"local[2]\") # Run Spark locally using 2 CPU threads\n",
    "            .getOrCreate()\n",
    ")     \n",
    "\n",
    "# Print the version of Spark you're using (e.g., \"4.0.0\")\n",
    "print(\"✅ Spark is ready. Version:\", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bafba95-4e52-4be0-a657-14eff633e258",
   "metadata": {},
   "source": [
    "### 04.02 Read Parquet Files into Spark\n",
    "Read a non-partitioned Parquet file into Spark. Measure the time taken. Also look at the execution plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56b586e0-1b2e-41ed-ae37-fbb93af4eff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+----------+--------+-----+---------------+\n",
      "| ID|Customer| Product|      Date|Quantity| Rate|           Tags|\n",
      "+---+--------+--------+----------+--------+-----+---------------+\n",
      "|  1|   Apple|Keyboard|2019/11/21|       5|31.15|Discount:Urgent|\n",
      "|  2|LinkedIn| Headset|2019/11/25|       5| 36.9|  Urgent:Pickup|\n",
      "|  3|Facebook|Keyboard|2019/11/24|       5|49.89|           NULL|\n",
      "|  4|  Google|  Webcam|2019/11/07|       4|34.21|       Discount|\n",
      "|  5|LinkedIn|  Webcam|2019/11/21|       3|48.69|         Pickup|\n",
      "+---+--------+--------+----------+--------+-----+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "--------------------------EXPLAIN--------------------------\n",
      "== Parsed Logical Plan ==\n",
      "UnresolvedDataSource format: parquet, isStreaming: false, paths: 1 provided\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "ID: int, Customer: string, Product: string, Date: string, Quantity: int, Rate: double, Tags: string\n",
      "Relation [ID#0,Customer#1,Product#2,Date#3,Quantity#4,Rate#5,Tags#6] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Relation [ID#0,Customer#1,Product#2,Date#3,Quantity#4,Rate#5,Tags#6] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet [ID#0,Customer#1,Product#2,Date#3,Quantity#4,Rate#5,Tags#6] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/bing/Downloads/Spark/Ex_Files_Big_Data_Analytics_Hadoop_Ap..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ID:int,Customer:string,Product:string,Date:string,Quantity:int,Rate:double,Tags:string>\n",
      "\n",
      "-------------------------END EXPLAIN-----------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sales_parquet = spark\\\n",
    "                .read\\\n",
    "                .parquet(\"dummy_hdfs/raw_parquet\")\n",
    "\n",
    "#Display the results\n",
    "sales_parquet.show(5)\n",
    "\n",
    "#show the execution plan\n",
    "print(\"\\n--------------------------EXPLAIN--------------------------\")\n",
    "sales_parquet.explain(True)\n",
    "print(\"-------------------------END EXPLAIN-----------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97a5807",
   "metadata": {},
   "source": [
    "Interpreting the Spark Execution Plan for a Non-Partitioned Parquet File\n",
    "\n",
    "- **Parsed Logical Plan**: Spark has detected a read from a data source (in this case, **Parquet**) with one provided path. This is the raw plan before schema resolution or optimization.\n",
    "\n",
    "- **Analyzed Logical Plan**: Spark resolves column names and their types. Here, the schema is successfully inferred with columns like `ID`, `Customer`, `Product`, etc., along with their respective data types. This confirms the file was properly read and schema was successfully applied.\n",
    "\n",
    "- **Optimized Logical Plan**: Spark applies logical optimizations (e.g., removing unnecessary projections or filters). In this case, the plan remains unchanged, as there are no filters or transformations to optimize.\n",
    "\n",
    "- **Physical Plan**: This is the actual execution strategy Spark will use:\n",
    "  - `FileScan parquet`: Spark is scanning a Parquet file using vectorized (batched) reads for performance.\n",
    "  - `ColumnarToRow`: Converts data from a columnar format (Parquet) into Spark's internal row format for further processing.\n",
    "  - `PushedFilters` and `PartitionFilters` are empty because the file was **not partitioned**, and no filtering was applied at read time.\n",
    "\n",
    "This plan confirms that Spark is performing a direct, full read of the non-partitioned Parquet file, with no pruning or predicate pushdown optimizations in place.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091d0a8d-0874-49f4-97e1-b6a9992fff31",
   "metadata": {},
   "source": [
    "### 04.03. Read Partitioned Data into Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41627f71-5748-4aa4-8e43-93a3ac51daf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/06 19:03:22 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: dummy_hdfs/partitioned_parquet/*.\n",
      "java.io.FileNotFoundException: File dummy_hdfs/partitioned_parquet/* does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:457)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:306)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+--------+-----+--------------------+\n",
      "| ID|Customer|      Date|Quantity| Rate|                Tags|\n",
      "+---+--------+----------+--------+-----+--------------------+\n",
      "|  6|  Google|2019/11/23|       5|40.58|                NULL|\n",
      "|  8|  Google|2019/11/13|       1|46.79|Urgent:Discount:P...|\n",
      "| 14|   Apple|2019/11/09|       4|40.27|            Discount|\n",
      "| 15|   Apple|2019/11/25|       5|38.89|                NULL|\n",
      "| 20|LinkedIn|2019/11/25|       4|36.77|       Urgent:Pickup|\n",
      "+---+--------+----------+--------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "--------------------------EXPLAIN--------------------------\n",
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet [ID#30,Customer#31,Date#32,Quantity#33,Rate#34,Tags#35] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(5 paths)[file:/Users/bing/Downloads/Spark/Ex_Files_Big_Data_Analytics_Hadoop_Ap..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ID:int,Customer:string,Date:string,Quantity:int,Rate:double,Tags:string>\n",
      "\n",
      "\n",
      "-------------------------END EXPLAIN-----------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_partitioned = spark\\\n",
    "                    .read\\\n",
    "                    .parquet(\"dummy_hdfs/partitioned_parquet/*\")\n",
    "\n",
    "#Display the results\n",
    "sales_partitioned.show(5)\n",
    "\n",
    "#show the execution plan\n",
    "print(\"\\n--------------------------EXPLAIN--------------------------\")\n",
    "sales_partitioned.explain()\n",
    "print(\"-------------------------END EXPLAIN-----------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482b0e13",
   "metadata": {},
   "source": [
    "Reading a partitioned parquet file was much much faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45899b52-23cc-4588-bb64-96d055f47fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+--------+-----+--------------------+\n",
      "| ID|Customer|      Date|Quantity| Rate|                Tags|\n",
      "+---+--------+----------+--------+-----+--------------------+\n",
      "|  2|LinkedIn|2019/11/25|       5| 36.9|       Urgent:Pickup|\n",
      "| 10|LinkedIn|2019/11/09|       2|26.91|Urgent:Discount:P...|\n",
      "| 11|Facebook|2019/11/26|       5|45.84|       Urgent:Pickup|\n",
      "| 12|  Google|2019/11/05|       2|41.17|     Discount:Urgent|\n",
      "| 17|   Apple|2019/11/09|       4|29.98|     Discount:Urgent|\n",
      "+---+--------+----------+--------+-----+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "#Read specific partition only\n",
    "sales_headset = spark\\\n",
    "                    .read\\\n",
    "                    .parquet(\"dummy_hdfs/partitioned_parquet/Product=Headset\")\n",
    "sales_headset.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd802eb-2b9e-4413-aba9-dbf0a105fcec",
   "metadata": {},
   "source": [
    "### 04.04 Read Bucketed Data into Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86dfacb8-abdf-4e62-a744-7aa9e66aaa97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/06 19:11:12 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: spark-warehouse/product_bucket_table/*.\n",
      "java.io.FileNotFoundException: File spark-warehouse/product_bucket_table/* does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:457)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:306)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+----------+--------+-----+---------------+\n",
      "| ID|Customer| Product|      Date|Quantity| Rate|           Tags|\n",
      "+---+--------+--------+----------+--------+-----+---------------+\n",
      "|  1|   Apple|Keyboard|2019/11/21|       5|31.15|Discount:Urgent|\n",
      "|  3|Facebook|Keyboard|2019/11/24|       5|49.89|           NULL|\n",
      "|  4|  Google|  Webcam|2019/11/07|       4|34.21|       Discount|\n",
      "|  5|LinkedIn|  Webcam|2019/11/21|       3|48.69|         Pickup|\n",
      "|  7|LinkedIn|  Webcam|2019/11/20|       4|37.19|           NULL|\n",
      "+---+--------+--------+----------+--------+-----+---------------+\n",
      "only showing top 5 rows\n",
      "+---+--------+-------+----------+--------+-----+---------------+\n",
      "| ID|Customer|Product|      Date|Quantity| Rate|           Tags|\n",
      "+---+--------+-------+----------+--------+-----+---------------+\n",
      "|  4|  Google| Webcam|2019/11/07|       4|34.21|       Discount|\n",
      "|  5|LinkedIn| Webcam|2019/11/21|       3|48.69|         Pickup|\n",
      "|  7|LinkedIn| Webcam|2019/11/20|       4|37.19|           NULL|\n",
      "|  9|  Google| Webcam|2019/11/10|       4|27.48|Discount:Urgent|\n",
      "| 18|LinkedIn| Webcam|2019/11/06|       3|40.59|       Discount|\n",
      "+---+--------+-------+----------+--------+-----+---------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "#Spark does not persist the Hive catalog between multiple Sparksession instances\n",
    "#You can additionally use a Hive metastore if you want to persist catalog\n",
    "#across SparkSession instances\n",
    "\n",
    "#Read the bucketed table directly from disk\n",
    "sales_bucketed = spark\\\n",
    "                    .read\\\n",
    "                    .parquet(\"spark-warehouse/product_bucket_table/*\")\n",
    "\n",
    "sales_bucketed.show(5)\n",
    "\n",
    "#Convert into a temporary view\n",
    "sales_bucketed.createOrReplaceTempView(\"product_bucket_table\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM product_bucket_table WHERE Product='Webcam'\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceb6163",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "- Spark programs run on a driver node, which works with Spark clusters to execute them\n",
    "- The driver can be thought of as the \"brain\" of the Spark application\n",
    "- Executors are the workers who do the actual data processing\n",
    "- One cluster can consist of multiple executor nodes capable of executing the program in parallel\n",
    "- When Data is loaded, it is converted to a dataframe or a resilient distributed data set (RDD), and during this conversion it is partitioned and individual partitions are assigned and moved into the executor nodes available\n",
    "- When a transform operation is executed, these operations are pushed down to the executors, who execute the code locally on their partitions and create new ones with the result\n",
    "- No data is moved between executors, and so transforms can be executed in parallel\n",
    "- During shuffling as a result of groupby or reduce, the executors need to move data back and forth from one another\n",
    "- Finally, data is collected back to the driver node and the partitions are merged and sent back to the driver\n",
    "- From there, they can be stored in an external database\n",
    "- Spark automatically optimizes the query plan, thanks to it's built in optimizer called Catalyst\n",
    "\n",
    "- Spark is powerful because it supports both traditional DataFrame operations and SQL, giving users flexible ways to work with data. Its true utility lies in its ability to parallelize computation across a cluster, making it much faster and more efficient for handling large-scale data. \n",
    "\n",
    "Resilient Distributed Dataset is the core **data structure** in Apache Spark\n",
    "- Immutable, distributed collection of objects (like a list or array)\n",
    "- Spread across multiple machines in a cluster\n",
    "- Designed for fault-tolerant and parallel computation \n",
    "- Paruqet and Avro are *read into* RDDs or Dataframes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b43b4c1",
   "metadata": {},
   "source": [
    "### Understanding Parallelism in Spark Clusters\n",
    "\n",
    "The number of **parallel operations** (i.e., tasks that can run simultaneously) in a Spark cluster is determined by:\n",
    "\n",
    "`# of executor nodes × # of CPU cores per executor`\n",
    "\n",
    "For example, if you have:\n",
    "- 4 executor nodes\n",
    "- Each node has 5 CPU cores\n",
    "\n",
    "Then Spark can run up to **20 parallel tasks** at a time.\n",
    "\n",
    "\n",
    "What happens when there are more partitions?\n",
    "\n",
    "If the **number of partitions in your data exceeds this maximum parallelism**, Spark will:\n",
    "- **Max out parallelism**, running tasks in parallel up to the available cores\n",
    "- **Queue the remaining tasks**, which run as earlier tasks complete\n",
    "\n",
    "This allows for **efficient utilization of cluster resources** — but does not exceed the system's physical limits.\n",
    "\n",
    "\n",
    "\n",
    "Contention with Other Jobs\n",
    "\n",
    "Keep in mind:\n",
    "- Spark is not the only thing running in most real-world environments.\n",
    "- If other Spark jobs or applications are running at the same time, they will **compete for the same executor and core resources**.\n",
    "- This can lead to **slower execution** or **resource starvation** if not managed properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a0208d",
   "metadata": {},
   "source": [
    "\"Spark only executes code when an action such as Reduce or Collect is performed.\"\n",
    "- Spark delays execution until it knows exactly what you want as a final result\n",
    "- Instead of going straight from the jump and going through all the operations, Spark optimizes the plan and then a physical execution strategy\n",
    "- Analogy: adding items into a cart (transformations) but nothing is actually delivered until we click \"Place Order\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecc-bio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
